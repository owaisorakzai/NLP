{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/owaisorakzai/NLP/blob/main/Transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oaWe_mIx9MPP"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import math,copy\n",
        "import warnings\n",
        "from torch import Tensor, nn\n",
        "import torch.optim as optim\n",
        "import spacy\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "import torch.optim as optim\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 623
        },
        "id": "_N5owK_4_Plv",
        "outputId": "5e32e24c-1965-457b-cd4e-c1b16214cd46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch==1.8.0\n",
            "  Downloading torch-1.8.0-cp37-cp37m-manylinux1_x86_64.whl (735.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 735.5 MB 13 kB/s \n",
            "\u001b[?25hCollecting torchtext==0.9.0\n",
            "  Downloading torchtext-0.9.0-cp37-cp37m-manylinux1_x86_64.whl (7.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.1 MB 31.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0) (4.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.0) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.0) (4.64.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0) (2022.5.18.1)\n",
            "Installing collected packages: torch, torchtext\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.11.0+cu113\n",
            "    Uninstalling torch-1.11.0+cu113:\n",
            "      Successfully uninstalled torch-1.11.0+cu113\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.12.0\n",
            "    Uninstalling torchtext-0.12.0:\n",
            "      Successfully uninstalled torchtext-0.12.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.12.0+cu113 requires torch==1.11.0, but you have torch 1.8.0 which is incompatible.\n",
            "torchaudio 0.11.0+cu113 requires torch==1.11.0, but you have torch 1.8.0 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.8.0 torchtext-0.9.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch",
                  "torchtext"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "\n",
        "!pip install -U torch==1.8.0 torchtext==0.9.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8dMNqEl2O_rd"
      },
      "outputs": [],
      "source": [
        "urdu_txt = open(\"Urdu.txt\", encoding=\"utf8\").read().split(\"\\n\")\n",
        "eng_txt = open(\"English.txt\", encoding=\"utf8\").read().split(\"\\n\")\n",
        "raw_data = {\n",
        "    \"Urdu\": [line for line in urdu_txt[1:100]],\n",
        "    \"English\": [line for line in eng_txt[1:100]],\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(raw_data, columns=[\"English\", \"Urdu\"])\n",
        "\n",
        "# create train and test set\n",
        "train, test = train_test_split(df, test_size=0.1)\n",
        "train.to_json(\"train.json\", orient=\"records\", lines=True)\n",
        "test.to_json(\"test.json\", orient=\"records\", lines=True)\n",
        "\n",
        "train.to_csv(\"train.csv\", index=False)\n",
        "test.to_csv(\"test.csv\", index=False)\n",
        "spacy_eng = spacy.load(\"en\")\n",
        "spacy_ur = spacy.blank('ur')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GdJ1J0c79hNL",
        "outputId": "3293a794-1577-4d36-a785-1d12c154dd12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                              English  \\\n",
            "0                                             English   \n",
            "1             To show anger after getting embarrassed   \n",
            "2                           A wolf in lamb's clothing   \n",
            "3               Cut your coat according to your cloth   \n",
            "4   Success is not final, failure is not fatal: it...   \n",
            "..                                                ...   \n",
            "85                               Old mare, red reigns   \n",
            "86                  Pure gold does not fear the flame   \n",
            "87  The enemy of a friend is an enemy; the enemy o...   \n",
            "88               To go about the same old beaten path   \n",
            "89                         Life is not a bed of roses   \n",
            "\n",
            "                                                 Urdu  \n",
            "0                                                Urdu  \n",
            "1                              کھسیانی بلی کھمبا نوچے  \n",
            "2                         بخل میں چھری منہ پے رام رام  \n",
            "3                        جتنی چادر ہو اتنا پیر پھیلاو  \n",
            "4   نہ کامیابی حتمی ہوتی ہے اور نہ ہی ناکامی: بلکہ...  \n",
            "..                                                ...  \n",
            "85                               بوڑھی گھوڑی لال لگام  \n",
            "86                                  سانچ کو آنچ نہیں  \n",
            "87               دوست کا دشمن دشمن، دشمن کا دشمن دوست  \n",
            "88                                       لکیر کا فقیر  \n",
            "89                           زندگی پھولوں کی سیج نہیں  \n",
            "\n",
            "[90 rows x 2 columns]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import random\n",
        "import re\n",
        "import spacy\n",
        "from torchtext.legacy import data\n",
        "from spacy.tokenizer import Tokenizer\n",
        "from spacy.lang.ur import Urdu\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "random.seed(0)\n",
        "df = pd.read_csv(\"train.csv\",names=[\"English\",\"Urdu\"])\n",
        "df['English'].replace('', np.nan, inplace=True)\n",
        "df['Urdu'].replace('', np.nan, inplace=True)\n",
        "df.dropna(subset=['English'], inplace=True)\n",
        "df.dropna(subset=['Urdu'], inplace=True)\n",
        "print(df)\n",
        "'''\n",
        "First :\n",
        "python -m spacy download en_core_web_sm\n",
        "'''\n",
        "spacy_eng = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "ar = Urdu()\n",
        "ar_Tokenizer = Tokenizer(ar.vocab)\n",
        "\n",
        "def engTokenizer(text):\n",
        " return  [word.text for word in spacy_eng.tokenizer(text)] \n",
        "\n",
        "def arTokenizer(sentence):\n",
        "    return  [word.text for word in \n",
        "             ar_Tokenizer(re.sub(r\"\\s+\",\" \",re.sub(r\"[\\.\\'\\\"\\n+]\",\" \",sentence)).strip())]\n",
        "\n",
        "\n",
        "\n",
        "SRC = data.Field(tokenize=arTokenizer,batch_first=False,tokenizer_language=\"ar\",init_token=\"ببدأ\",eos_token=\"نهها\")\n",
        "TRG = data.Field(tokenize=engTokenizer,batch_first=False,init_token=\"<sos>\",eos_token=\"<eos>\")\n",
        " \n",
        "\n",
        "class TextDataset(data.Dataset):\n",
        "\n",
        "    def __init__(self, df, src_field, target_field, is_test=False, **kwargs):\n",
        "        fields = [('eng',target_field ), ('ar',src_field)]\n",
        "        samples = []\n",
        "        for i, row in df.iterrows():\n",
        "            eng = row.English \n",
        "            ar = row.Urdu\n",
        "            samples.append(data.Example.fromlist([eng, ar], fields))\n",
        "\n",
        "        super().__init__(samples, fields, **kwargs)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.samples[idx]\n",
        "\n",
        "torchdataset = TextDataset(df,SRC,TRG)\n",
        "\n",
        "train_data, valid_data = torchdataset.split(split_ratio=0.8, random_state = random.seed(0))\n",
        "\n",
        "SRC.build_vocab(train_data,min_freq=2)\n",
        "TRG.build_vocab(train_data,min_freq=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "avzIwMMO-1Qu"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VnW0FV1j9JCj"
      },
      "outputs": [],
      "source": [
        "class Embedder(torch.nn.Module):\n",
        "    \n",
        "    def __init__(self, vocab_size, d_model):\n",
        "        super().__init__()\n",
        "        self.embed = torch.nn.Embedding(vocab_size, d_model)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return self.embed(x)\n",
        "\n",
        "import math\n",
        "\n",
        "class PositionalEncoder(torch.nn.Module):\n",
        "    \n",
        "    def __init__(self, d_model, max_seq_len=80):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        \n",
        "        pe_matrix = torch.zeros(max_seq_len, d_model)\n",
        "        \n",
        "        for pos in range(max_seq_len):\n",
        "            for i in range(0, d_model, 2):\n",
        "                pe_matrix[pos, i] = math.sin(pos/10000**(2*i/d_model))\n",
        "                pe_matrix[pos, i+1] = math.cos(pos/10000**(2*i/d_model))\n",
        "        pe_matrix = pe_matrix.unsqueeze(0) \n",
        "        self.register_buffer('pe', pe_matrix)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        seq_len = x.size()[1]\n",
        "        x = x + self.pe[:, :seq_len]\n",
        "        return x\n",
        "\n",
        "\n",
        "import math\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def scaled_dot_product_attention(q, k, v, mask=None, dropout=None):\n",
        "    attention_scores = torch.matmul(q, k.transpose(-2, -1))/math.sqrt(q.shape[-1])\n",
        "    \n",
        "\n",
        "    if mask is not None:\n",
        "        attention_scores = attention_scores.masked_fill(mask == 0, value=-1e9)\n",
        "        \n",
        "    attention_weights = F.softmax(attention_scores, dim=-1)\n",
        "    \n",
        "    if dropout is not None:\n",
        "        attention_weights = dropout(attention_weights)\n",
        "        \n",
        "    output = torch.matmul(attention_weights, v)\n",
        "    return output\n",
        "\n",
        "class MultiHeadAttention(torch.nn.Module):\n",
        "    def __init__(self, n_heads, d_model, dropout=0.1):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.n_heads = n_heads\n",
        "        self.d_model = d_model\n",
        "        self.d_k = self.d_v = d_model//n_heads\n",
        "        self.q_linear_layers = []\n",
        "        self.k_linear_layers = []\n",
        "        self.v_linear_layers = []\n",
        "        for i in range(n_heads):\n",
        "            self.q_linear_layers.append(torch.nn.Linear(d_model, self.d_k))\n",
        "            self.k_linear_layers.append(torch.nn.Linear(d_model, self.d_k))\n",
        "            self.v_linear_layers.append(torch.nn.Linear(d_model, self.d_v))\n",
        "        \n",
        "        self.dropout = torch.nn.Dropout(dropout)\n",
        "        self.out = torch.nn.Linear(n_heads*self.d_v, d_model)\n",
        "        \n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        multi_head_attention_outputs = []\n",
        "        for q_linear, k_linear, v_linear in zip(self.q_linear_layers,\n",
        "                                                self.k_linear_layers,\n",
        "                                                self.v_linear_layers):\n",
        "            new_q = q_linear(q)\n",
        "            new_k = k_linear(k)\n",
        "            new_v = v_linear(v)\n",
        "            head_v = scaled_dot_product_attention(new_q, new_k, new_v, mask, self.dropout)\n",
        "            multi_head_attention_outputs.append(head_v)\n",
        "        concat = torch.cat(multi_head_attention_outputs, -1)\n",
        "        output = self.out(concat)\n",
        "        \n",
        "        return output\n",
        "\n",
        "\n",
        "\n",
        "class FeedForward(torch.nn.Module):\n",
        "    def __init__(self, d_model, d_ff=2048, dropout=0.1):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.linear_1 = torch.nn.Linear(d_model, d_ff)\n",
        "        self.dropout = torch.nn.Dropout(dropout)\n",
        "        self.linear_2 = torch.nn.Linear(d_ff, d_model)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.dropout(F.relu(self.linear_1(x)))\n",
        "        x = self.linear_2(x)\n",
        "        return x\n",
        "\n",
        "class LayerNorm(torch.nn.Module):\n",
        "    def __init__(self, d_model, eps=1e-6):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.alpha = torch.nn.Parameter(torch.ones(self.d_model))\n",
        "        self.beta = torch.nn.Parameter(torch.zeros(self.d_model))\n",
        "        self.eps = eps\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x_hat = (x - x.mean(dim=-1, keepdim=True))/(x.std(dim=-1, keepdim=True) + self.eps)\n",
        "        x_tilde = self.alpha*x_hat + self.beta\n",
        "        return x_tilde\n",
        "class EncoderLayer(torch.nn.Module):\n",
        "    def __init__(self, d_model, n_heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.norm_1 = LayerNorm(d_model)\n",
        "        self.norm_2 = LayerNorm(d_model)\n",
        "        self.multi_head_attention = MultiHeadAttention(n_heads, d_model)\n",
        "        self.feed_forward = FeedForward(d_model)\n",
        "        self.dropout_1 = torch.nn.Dropout(dropout)\n",
        "        self.dropout_2 = torch.nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x, mask):\n",
        "        x = x + self.dropout_1(self.multi_head_attention(x, x, x, mask))\n",
        "        x = self.norm_1(x)\n",
        "        \n",
        "        x = x + self.dropout_2(self.feed_forward(x))\n",
        "        x = self.norm_2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "class DecoderLayer(torch.nn.Module):\n",
        "    def __init__(self, d_model, n_heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.norm_1 = LayerNorm(d_model)\n",
        "        self.norm_2 = LayerNorm(d_model)\n",
        "        self.norm_3 = LayerNorm(d_model)\n",
        "        \n",
        "        self.dropout_1 = torch.nn.Dropout(dropout)\n",
        "        self.dropout_2 = torch.nn.Dropout(dropout)\n",
        "        self.dropout_3 = torch.nn.Dropout(dropout)\n",
        "        \n",
        "        self.multi_head_attention_1 = MultiHeadAttention(n_heads, d_model)\n",
        "        self.multi_head_attention_2 = MultiHeadAttention(n_heads, d_model)\n",
        "        \n",
        "        self.feed_forward = FeedForward(d_model)\n",
        "        \n",
        "    def forward(self, x, encoder_output, src_mask, trg_mask):\n",
        "        x = self.dropout_1(self.multi_head_attention_1(x, x, x, trg_mask))\n",
        "        x = x + self.norm_1(x)\n",
        "        \n",
        "        x = self.dropout_2(self.multi_head_attention_2(x, encoder_output, encoder_output, src_mask))\n",
        "        x = x + self.norm_2(x)\n",
        "        \n",
        "        x = self.dropout_3(self.feed_forward(x))\n",
        "        x = x + self.norm_3(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "import copy\n",
        "\n",
        "def clone_layer(module, N):\n",
        "    return torch.nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n",
        "class Encoder(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, N, n_heads):\n",
        "        super().__init__()\n",
        "        self.embed = Embedder(vocab_size, d_model)\n",
        "        self.pe = PositionalEncoder(d_model)\n",
        "        self.encoder_layers = clone_layer(EncoderLayer(d_model, n_heads), N)\n",
        "        self.norm = LayerNorm(d_model)\n",
        "        \n",
        "    def forward(self, src, mask):\n",
        "        x = self.embed(src)\n",
        "        x = self.pe(x)\n",
        "        for encoder in self.encoder_layers:\n",
        "            x = encoder(x, mask)\n",
        "        return self.norm(x)\n",
        "\n",
        "\n",
        "class Decoder(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, N, n_heads):\n",
        "        super().__init__()\n",
        "        self.embed = Embedder(vocab_size, d_model)\n",
        "        self.pe = PositionalEncoder(d_model)\n",
        "        self.decoder_layers = clone_layer(DecoderLayer(d_model, n_heads), N)\n",
        "        self.norm = LayerNorm(d_model)\n",
        "        \n",
        "    def forward(self, trg, encoder_output, src_mask, trg_mask):\n",
        "        x = self.embed(trg)\n",
        "        x = self.pe(x)\n",
        "        for decoder in self.decoder_layers:\n",
        "            x = decoder(x, encoder_output, src_mask, trg_mask)\n",
        "        return self.norm(x)\n",
        "\n",
        "\n",
        "\n",
        "class Transformer(torch.nn.Module):\n",
        "    def __init__(self, src_vocab_size, trg_vocab_size, d_model, N, n_heads):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(src_vocab_size, d_model, N, n_heads)\n",
        "        self.decoder = Decoder(trg_vocab_size, d_model, N, n_heads)\n",
        "        self.linear = torch.nn.Linear(d_model, trg_vocab_size)\n",
        "        \n",
        "    def forward(self, src, trg, src_mask, trg_mask):\n",
        "        encoder_output = self.encoder(src, src_mask)\n",
        "        decoder_output = self.decoder(trg, encoder_output, src_mask, trg_mask)\n",
        "        output = self.linear(decoder_output)\n",
        "        return output\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zAXcBhWQ9Ga7",
        "outputId": "86f8328e-6196-4a27-aa96-3bb6c0ba04c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of urdu vocabulary: 70\n",
            "Size of english vocabulary: 78\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:23: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n"
          ]
        }
      ],
      "source": [
        "d_model = 512\n",
        "n_heads = 8\n",
        "N = 6\n",
        "src_vocab_size = len(SRC.vocab)\n",
        "trg_vocab_size = len(TRG.vocab)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Size of urdu vocabulary:\",src_vocab_size)\n",
        "train_iter, valid_iter = data.BucketIterator.splits(\n",
        "    (train_data,valid_data), \n",
        "    batch_size = 16,\n",
        "    sort=None,\n",
        "    sort_within_batch=False,\n",
        "    sort_key=lambda x: len(x.eng),\n",
        "    device = device,\n",
        "    shuffle=True\n",
        ")\n",
        "trg_vocab_size =len(TRG.vocab)\n",
        "print(\"Size of english vocabulary:\",trg_vocab_size)\n",
        "pad_idx = SRC.vocab.stoi[\"<pad>\"]\n",
        "model = Transformer( trg_vocab_size,src_vocab_size, d_model, N, n_heads)\n",
        "for p in model.parameters():\n",
        "    if p.dim() > 1:\n",
        "        torch.nn.init.xavier_uniform(p)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
        "import numpy as np\n",
        "\n",
        "def create_mask(src_input, trg_input):\n",
        "    pad = SRC.vocab.stoi['<pad>']\n",
        "    src_mask = (src_input != pad).unsqueeze(1)\n",
        "    \n",
        "    # Target input mask\n",
        "    trg_mask = (trg_input != pad).unsqueeze(1)\n",
        "    \n",
        "    seq_len = trg_input.size(1)\n",
        "    nopeak_mask = np.tril(np.ones((1, seq_len, seq_len)), k=0).astype('uint8')\n",
        "    nopeak_mask = torch.from_numpy(nopeak_mask) != 0\n",
        "    trg_mask = trg_mask & nopeak_mask\n",
        "    \n",
        "    return src_mask, trg_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O58BzZgU8EIf",
        "outputId": "8b3fc736-334f-49a6-fbc6-a25e8be6feee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time = 0.14562635421752929, epoch = 1, iter = 1, loss = 4.582399845123291\n",
            "time = 0.08033700386683146, epoch = 1, iter = 2, loss = 5.814487934112549\n",
            "time = 0.053155708312988284, epoch = 1, iter = 3, loss = 3.2108092308044434\n",
            "time = 0.03178740342458089, epoch = 1, iter = 4, loss = 3.3721091747283936\n",
            "time = 0.025288299719492594, epoch = 1, iter = 5, loss = 3.4064455032348633\n",
            "time = 0.02400938669840495, epoch = 2, iter = 1, loss = 2.697145938873291\n",
            "time = 0.030090534687042238, epoch = 2, iter = 2, loss = 3.408078908920288\n",
            "time = 0.03930020332336426, epoch = 2, iter = 3, loss = 2.696653127670288\n",
            "time = 0.06033289432525635, epoch = 2, iter = 4, loss = 2.9975969791412354\n",
            "time = 0.04249429702758789, epoch = 2, iter = 5, loss = 3.4456493854522705\n",
            "time = 0.04225571950276693, epoch = 3, iter = 1, loss = 2.8997251987457275\n",
            "time = 0.031193641821543376, epoch = 3, iter = 2, loss = 3.281088352203369\n",
            "time = 0.031675843397776286, epoch = 3, iter = 3, loss = 2.7641992568969727\n",
            "time = 0.05299049218495687, epoch = 3, iter = 4, loss = 3.0136327743530273\n",
            "time = 0.058017031351725264, epoch = 3, iter = 5, loss = 2.9957079887390137\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import time\n",
        "\n",
        "def train_model(n_epochs, output_interval=100):\n",
        "    model.train()\n",
        "    start = time.time()\n",
        "    \n",
        "    for epoch in range(n_epochs):\n",
        "        \n",
        "        total_loss = 0\n",
        "        for i, batch in enumerate(train_iter):\n",
        "            \n",
        "            src_input = batch.eng.transpose(0, 1)\n",
        "            trg = batch.ar.transpose(0, 1) \n",
        "            \n",
        "            trg_input = trg[:, :-1]\n",
        "            ys = trg[:, 1:].contiguous().view(-1)\n",
        "            \n",
        "            src_mask, trg_mask = create_mask(src_input, trg_input)\n",
        "            preds = model(src_input, trg_input, src_mask, trg_mask)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            loss = F.cross_entropy(preds.view(-1, preds.size(-1)), ys, ignore_index=1)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            total_loss += loss.data\n",
        "\n",
        "            if (i + 1) % output_interval == 0:\n",
        "                avg_loss = total_loss/output_interval\n",
        "                print('time = {}, epoch = {}, iter = {}, loss = {}'.format((time.time() - start)/60,\n",
        "                                                                           epoch + 1,\n",
        "                                                                           i + 1,\n",
        "                                                                           avg_loss))\n",
        "                total_loss = 0\n",
        "                start = time.time()\n",
        "train_model(3, output_interval=1)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "muRp18NH9DoK"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Transformers.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNQIaF6cI711Pgh4Be1tKdK",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}